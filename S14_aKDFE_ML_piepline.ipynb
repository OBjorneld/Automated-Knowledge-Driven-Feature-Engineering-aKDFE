{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC9PAlnS1Sgn"
      },
      "outputs": [],
      "source": [
        "#########################################\n",
        "#Latest version for S14-experiment\n",
        "#########################################\n",
        "\n",
        "#Installation\n",
        "init_load=False\n",
        "if init_load:\n",
        "  !pip install sklearn-relief==1.0.0b2\n",
        "  !pip install skrebate\n",
        "  !pip install sklearn-genetic\n",
        "  !pip install git+https://github.com/smazzanti/mrmr\n",
        "  !pip install shap\n",
        "  !pip install Probatus\n",
        "\n",
        "# Basics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "#Preprocessing\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from operator import itemgetter\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import lightgbm\n",
        "#import shap\n",
        "\n",
        "#GPU\n",
        "import tensorflow as tf\n",
        "#tf.test.gpu_device_name()\n",
        "\n",
        "#Feature selection methods\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "import sklearn_relief as relief\n",
        "from skrebate import ReliefF\n",
        "#from mrmr import mrmr_classif\n",
        "from probatus.feature_elimination import ShapRFECV\n",
        "from sklearn.feature_selection import RFE # Recursive feature elimination\n",
        "from sklearn.feature_selection import RFECV # with crossvalidation\n",
        "from sklearn.svm import SVC\n",
        "from genetic_selection import GeneticSelectionCV\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "#Classification methods\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#Evaluation\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn import model_selection\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "#Visualization\n",
        "%matplotlib inline\n",
        "from IPython.display import HTML, display\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#OS\n",
        "#Path.home()\n",
        "#uploaded = files.upload()\n",
        "\n",
        "#print all rows\n",
        "pd.set_option('display.max_rows', 20)\n",
        "\n",
        "#Grouped result\n",
        "  #grouped_res = exp_result.groupby(['model', 'FS_method', 'Normalization']).size()\n",
        "\n",
        "def LoadGDriveFile (project, filename):\n",
        "  drive.mount('/content/drive')\n",
        "  file_path='/content/drive/MyDrive/PHD/'+project+'/dataset/'+filename\n",
        "  !cp \"/content/drive/MyDrive/PHD/S14/dataset/S14_TOTAL_DATASET_V2.csv\" \"S14_TOTAL_DATASET_V2.csv\"\n",
        "  #!cp file_path {filename}\n",
        "\n",
        "#Mount Google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def print_border(type, length=70):\n",
        "  border = ''\n",
        "  for i in range(length+1):\n",
        "    border=border+type\n",
        "  print (border)\n",
        "\n",
        "def make_csv_df(dataset,y, sep, imp_mean, split, ratio, featuregroup, dataset_type):\n",
        "    X_train_pd=[]\n",
        "    X_test_pd=[]\n",
        "    y_train_pd=[]\n",
        "    y_test_pd=[]\n",
        "    #OrgConcept=[51131148, 51126540,51136012]\n",
        "    df = pd.read_csv(dataset,sep=sep, low_memory = False)\n",
        "    rows =len(df)\n",
        "    print ('Selected dataset type and feature group:', dataset_type,'and' ,featuregroup)\n",
        "    df=df.head(rows)\n",
        "    if dataset_type=='RAW':\n",
        "      if ('PK' in df.columns):\n",
        "        df=df.drop(columns=['PK'])\n",
        "      if (featuregroup in (['ORG','IDEATION' ,'FE-GEN'])):\n",
        "        df=df[df['CREATION_PHASE']==featuregroup]\n",
        "        print(featuregroup, 'chosen')\n",
        "      if (featuregroup=='ORGIDEA'):\n",
        "        print('ORGIDEA chosen')\n",
        "        df=df[df['CREATION_PHASE'].isin(['ORG','IDEATION'])]\n",
        "      if (featuregroup=='ORGFE'):\n",
        "        print('ORGFE chosen')\n",
        "        df=df[df['CREATION_PHASE'].isin(['ORG','FE-GEN'])]\n",
        "      display(df.groupby(['CREATION_PHASE', 'CONCEPT_ID'])['PID'].count() )\n",
        "      print('Unique F-group and unique patients:')\n",
        "      display(df[['CREATION_PHASE', 'PID']].drop_duplicates().groupby(['CREATION_PHASE'])['PID'].count() )\n",
        "      df=df.drop(columns=['CREATION_PHASE'])\n",
        "      df=df.drop(columns=['CONCEPT_ID'])\n",
        "    if dataset_type=='TIDY':\n",
        "      print ('chosen TIDY')\n",
        "      if ('PK' in df.columns):\n",
        "        df=df.drop(columns=['PK'])\n",
        "      if (featuregroup=='ORG'):\n",
        "        df=df[org_list+y]\n",
        "        print('ORG chosen')\n",
        "      if (featuregroup=='IDEATION'):\n",
        "        df=df[idea_list+y]\n",
        "        print('IDEATION chosen')\n",
        "      if (featuregroup=='FE-GEN'):\n",
        "        df=df.drop(columns=org_list+idea_list)\n",
        "        print('FE-GEN chosen')\n",
        "      if (featuregroup=='ORGIDEA'):\n",
        "        df= df[org_list+idea_list+y]\n",
        "        print('ORGIDEA chosen')\n",
        "      if (featuregroup=='ORGFE'):\n",
        "        df=df.drop(columns=idea_list)\n",
        "        print('ORGFE chosen')\n",
        "    y_pd=df[y[0]]\n",
        "    X_pd=df.drop(columns=[y[0]])\n",
        "    #Replacing NaN with meanvalues\n",
        "    if (imp_mean):\n",
        "      X_pd=X_pd.fillna(X_pd.mean())\n",
        "      y_pd=y_pd.fillna(y_pd.mean())\n",
        "    #Drop all NaN /zero columns\n",
        "    X_pd.dropna(how='all', axis=1, inplace=True)\n",
        "    #Summary of dataset\n",
        "    X_var=X_pd.columns.values\n",
        "    print (\"Imported number of rows: \", rows,\"of\",rows, \"from dataset:\", dataset)\n",
        "    print ('The dataset has based on featuregroup and dataset type selected:',len(X_var), 'describing features and',len(y), 'dependent variabel:')\n",
        "    print ('The describing features are: \\n',X_var)\n",
        "    print ('The dependent variable is:',y[0])\n",
        "    #Splitting into train and test dataset\n",
        "    if (split):\n",
        "      X_train, X_test, y_train_list, y_test_list = train_test_split(X_pd, y_pd, test_size=ratio, random_state=8675309)\n",
        "    y_test= pd.DataFrame(y_test_list)\n",
        "    y_train= pd.DataFrame(y_train_list)\n",
        "    print ('Sixe of X_PD end create dataset', X_pd.shape)\n",
        "    return (X_pd, y_pd, X_train, X_test, y_train, y_test)\n",
        "\n",
        "\n",
        "def dfNorm(df, type):\n",
        "    scaler = preprocessing.MinMaxScaler()\n",
        "    columns=sorted(df)\n",
        "    df=pd.DataFrame(scaler.fit_transform(pd.DataFrame(df)))\n",
        "    df.columns=columns\n",
        "    return (df)\n",
        "\n",
        "def var_typing (df):\n",
        "    i = 0\n",
        "    df['f_code']='-1'\n",
        "    for i in range (len(df)) :\n",
        "      feature=df['feature'].iloc[i]\n",
        "      if (feature[0:1] =='X'):\n",
        "        feature_code = 'ORG'\n",
        "      else:\n",
        "       feature_code = 'FE'\n",
        "      df['f_code'].iloc[i]=feature_code\n",
        "      i+=1\n",
        "\n",
        "def actual_time():\n",
        "  now = datetime.now()\n",
        "  current_time = now.strftime(\"%H:%M:%S\")\n",
        "  return current_time\n",
        "\n",
        "def time_diff(starttime, endtime):\n",
        "  if starttime == 0:\n",
        "    tmdelta=actual_time()\n",
        "  else:\n",
        "    period=round(endtime-starttime)\n",
        "    tmdelta= str(timedelta(seconds = period))\n",
        "  return tmdelta\n",
        "\n",
        "def make_PID_unique_df(result_df):\n",
        "  result_UnPID_df=result_df[['PID','Y1', 'Y_mv']].drop_duplicates()\n",
        "  print ('Shape of unique PID dataset:',result_UnPID_df.shape, 'versus input', result_df.shape )\n",
        "  return result_UnPID_df\n",
        "\n",
        "def add_y_major_vote(result_UnPID_df, group_col, calc_col):\n",
        "  Y_hat_mean=result_UnPID_df.groupby('PID')['Y_hat'].mean()\n",
        "  Y_hat_mean=Y_hat_mean.rename(\"Y_hat_mean\")\n",
        "  Y_hat_mean=Y_hat_mean.to_frame()\n",
        "  y_pred_hat_mean=pd.merge(result_UnPID_df, Y_hat_mean, how='inner', on='PID')\n",
        "  y_pred_mv=y_pred_hat_mean['Y_hat_mean'].round()\n",
        "  result_UnPID_df=y_pred_hat_mean.assign(Y_mv=y_pred_mv)\n",
        "  if reporting:\n",
        "    print ('Rows with means >0 AND < 1',Y_hat_mean.loc[lambda x : (x < 1) & (x > 0)] )\n",
        "    print (y_pred_hat_mean.head(10))\n",
        "    print ('Rows with Y_hat mean > 0 AND <1', result_UnPID_df.loc[(result_UnPID_df['Y_hat_mean']>0) & (result_UnPID_df['Y_hat_mean']<1)].drop_duplicates() )\n",
        "  return result_UnPID_df\n",
        "\n",
        "def FSVarianceThreshold(X, FS_X_train, FS_X_test,y_pd,n):\n",
        "    threshold = 0\n",
        "    X_VT_input = X.select_dtypes(include='number')\n",
        "    FS_VT=VarianceThreshold(threshold=threshold)\n",
        "    X_VT=FS_VT.fit_transform(X_VT_input)\n",
        "    print ('Size of Variance threshold ds before masking and type:',X_VT.shape)\n",
        "    #display('FS_VT before and size, type:',FS_VT, np.size(FS_VT), type(FS_VT))\n",
        "    X_VT_variance=FS_VT.variances_\n",
        "    #display('List of variances: ',X_VT_variance)\n",
        "    drop_columns=X_VT_variance.argsort()[:-n]\n",
        "    sorted_var=np.sort(FS_VT.variances_)\n",
        "    sorted_var=sorted_var[sorted_var.argsort()[-n:]]\n",
        "    X_VT_mask=FS_VT.get_support()\n",
        "\n",
        "    for i in range(len(drop_columns)):\n",
        "      index=drop_columns[i]\n",
        "      X_VT_mask[index] = False\n",
        "\n",
        "    X_VT_df = X.loc[:, X_VT_mask]\n",
        "    X_VT_train_df = FS_X_train.loc[:, X_VT_mask]\n",
        "    X_VT_test_df = FS_X_test.loc[:, X_VT_mask]\n",
        "    return X_VT_df, X_VT_train_df, X_VT_test_df\n",
        "\n",
        "def KBestN(X,X_train, X_test,y,n):\n",
        "    FS_KBestChi2=SelectKBest(chi2, k=n)\n",
        "    X_KBChi2=FS_KBestChi2.fit_transform(X, y)\n",
        "    X_KBestChi2_mask=FS_KBestChi2.get_support()\n",
        "    X_KBChi2_df = X.loc[:, X_KBestChi2_mask]\n",
        "    X_KBChi2_train_df = X_train.loc[:,X_KBestChi2_mask]\n",
        "    X_KBChi2_test_df = X_test.loc[:,X_KBestChi2_mask]\n",
        "    print ('Dataframe with K Best Neighbours (Chi2) for n=',n,'best features')\n",
        "    return X_KBChi2_df, X_KBChi2_train_df, X_KBChi2_test_df\n",
        "\n",
        "def KBestMutual(FS_X,FS_X_train, FS_X_test,y,n):\n",
        "    print ('KBEstMut-func started')\n",
        "    FS_KBMutual=SelectKBest(mutual_info_classif, k=n)\n",
        "    X_KBMutual=FS_KBMutual.fit_transform(FS_X, y)\n",
        "    X_KBMutual_mask=FS_KBMutual.get_support()\n",
        "    X_KBMutual_df = FS_X.loc[:, X_KBMutual_mask]\n",
        "    X_KBMutual_train_df = FS_X_train.loc[:, X_KBMutual_mask]\n",
        "    X_KBMutual_test_df = FS_X_test.loc[:, X_KBMutual_mask]\n",
        "    print ('Dataframe with K Best Neighbours (Mutual info) for n=',n,'best features')\n",
        "    return X_KBMutual_df, X_KBMutual_train_df, X_KBMutual_test_df\n",
        "\n",
        "def Relief (FS_X,FS_X_train, FS_X_test,y,n):\n",
        "    y=np.reshape(y,len(y)) #change size from (x,1) to (x,)\n",
        "    FS_Relief=relief.Relief(n_features=n)\n",
        "    X_Relief=FS_Relief.fit_transform(FS_X, y)\n",
        "    X_Relief_mask=FS_Relief.get_support()\n",
        "    X_Relief_df = FS_X.loc[:, X_Relief_mask]\n",
        "    X_Relief_train_df = FS_X_train.loc[:, X_Relief_mask]\n",
        "    X_Relief_test_df = FS_X_test.loc[:, X_Relief_mask]\n",
        "    print ('Dataframe with Relief for n=',n,'best features')\n",
        "    return X_Relief_df, X_Relief_train_df, X_Relief_test_df\n",
        "\n",
        "def ReliefSB (FS_X,FS_X_train, FS_X_test,y,n):\n",
        "    y=np.reshape(y,len(y)) #change size from (x,1) to (x,)\n",
        "    FS_ReliefSB=ReliefF(n_features_to_select=n)\n",
        "    X_ReliefSB=FS_ReliefSB.fit_transform(FS_X, y)\n",
        "    X_ReliefSB_mask=FS_ReliefSB.get_support()\n",
        "    X_ReliefSB_df = FS_X.loc[:, X_ReliefSB_mask]\n",
        "    X_ReliefSB_train_df = FS_X_train.loc[:, X_ReliefSB_mask]\n",
        "    X_ReliefSB_test_df = FS_X_test.loc[:, X_ReliefSB_mask]\n",
        "    print ('Dataframe with ReliefSB for n=',n,'best features')\n",
        "    return X_ReliefSB_df, X_ReliefSB_train_df, X_ReliefSB_test_df\n",
        "\n",
        "def mRMR(FS_X,X_train, X_test,y,n):\n",
        "    FS_mRMR=mrmr_classif(K=n)\n",
        "    X_mRMR=FS_mRMR.fit_transform(FS_X,y)\n",
        "    X_mRMR_mask=FS_mRMR.get_support()\n",
        "    X_mRMR_df = FS_X.loc[:, X_mRMR_mask]\n",
        "    X_mRMR_train_df = FS_X.loc[:, X_mRMR_mask]\n",
        "    X_mRMR_test_df = FS_X.loc[:, X_mRMR_mask]\n",
        "    print ('Dataframe with mRMR for n=',n,'best features')\n",
        "    return X_mRMR_df\n",
        "\n",
        "def GenSelectnew(FS_X,FS_X_train, FS_X_test,y,n):\n",
        "    estimator = DecisionTreeClassifier()\n",
        "    iterations=100\n",
        "    FS_GenSelect=GeneticSelectionCV(estimator, cv=5, verbose=0, scoring=\"accuracy\", max_features=n,\n",
        "    n_population=100, crossover_proba=0.5, mutation_proba=0.2, n_generations=50, crossover_independent_proba=0.5,\n",
        "    mutation_independent_proba=0.04, tournament_size=3, n_gen_no_change=10, caching=True, n_jobs=-1)\n",
        "    X_GenSelect=FS_GenSelect.fit_transform(FS_X, y)\n",
        "    print(sum(FS_GenSelect.support_))\n",
        "    print(FS_GenSelect.support_)\n",
        "    print(FS_GenSelect.generation_scores_)\n",
        "    print(FS_GenSelect.generation_scores_)\n",
        "    print (dir(FS_GenSelect))\n",
        "    X_GenSelect_mask=FS_GenSelect.get_support()\n",
        "\n",
        "    X_GenSelect_df = FS_X.loc[:, X_GenSelect_mask]\n",
        "    X_GenSelect_train_df = FS_X_train.loc[:, X_GenSelect_mask]\n",
        "    X_GenSelect_test_df = FS_X_test.loc[:, X_GenSelect_mask]\n",
        "    print ('Dataframe with genetic selection for n=',n,'best features')\n",
        "    return X_GenSelect_df, X_GenSelect_train_df, X_GenSelect_test_df\n",
        "\n",
        "def RFE_FS(FS_X,FS_X_train, FS_X_test,y,n):\n",
        "    estimator =DecisionTreeClassifier()\n",
        "    FS_RFE = RFE(estimator=estimator, n_features_to_select=n, step=1, verbose=0)\n",
        "    X_RFE=FS_RFE.fit(FS_X, y)\n",
        "    X_RFE_mask=FS_RFE.get_support()\n",
        "    X_RFE_df = FS_X.loc[:, X_RFE_mask]\n",
        "    X_RFE_train_df = FS_X_train.loc[:, X_RFE_mask]\n",
        "    X_RFE_test_df = FS_X_test.loc[:, X_RFE_mask]\n",
        "    print ('Dataframe with Recursive feature elimination (RFE) for n=',n,'best features')\n",
        "    return X_RFE_df, X_RFE_train_df, X_RFE_test_df\n",
        "\n",
        "def RFECV_FS(FS_X,FS_X_train, FS_X_test,y,n):\n",
        "    RFE_cv=2 # Number of cross validations\n",
        "    estimator=DecisionTreeClassifier()\n",
        "    FS_RFECV = RFECV(estimator=estimator, min_features_to_select=n, cv=RFE_cv,step=1, verbose=0)\n",
        "    X_RFECV=FS_RFECV.fit(FS_X, y)\n",
        "    RFECV_rank = FS_RFECV.ranking_\n",
        "    X_RFECV_mask= np.empty(len(RFECV_rank), dtype=bool)\n",
        "    RFECV_rank_with_idx = enumerate(RFECV_rank)\n",
        "    RFECV_sorted_ranks_idx = sorted(RFECV_rank_with_idx, key=lambda x: x[1])\n",
        "    RFECV_top_n_idx = [idx for idx, rnk in RFECV_sorted_ranks_idx[:n]]\n",
        "    for i in range (len(X_RFECV_mask)):\n",
        "      X_RFECV_mask[i]=False\n",
        "    for i in range(len(RFECV_top_n_idx)):\n",
        "      index=RFECV_top_n_idx[i]\n",
        "      X_RFECV_mask[index] = True\n",
        "    X_RFECV_df = FS_X.loc[:, X_RFECV_mask]\n",
        "    X_RFECV_train_df = FS_X_train.loc[:, X_RFECV_mask]\n",
        "    X_RFECV_test_df = FS_X_test.loc[:, X_RFECV_mask]\n",
        "    print ('Dataframe with RFECV ready', X_RFECV_df.shape)\n",
        "    return X_RFECV_df, X_RFECV_train_df, X_RFECV_test_df\n",
        "\n",
        "def shapRFECV_FS(FS_X,FS_X_train, FS_X_test,y,n):\n",
        "    print ('Shape of y:', y.shape)\n",
        "    if (len (y.shape)==2):\n",
        "      print('Size not 1')\n",
        "      y=y.iloc[:,0]\n",
        "    clf = lightgbm.LGBMClassifier(max_depth=5, class_weight='balanced')\n",
        "    param_grid = {\n",
        "    'n_estimators': [5, 7, 10],\n",
        "    'num_leaves': [3, 5, 7, 10],\n",
        "    }\n",
        "    search = RandomizedSearchCV(clf, param_grid)\n",
        "    shapFS_RFECV = ShapRFECV(\n",
        "    clf=search, step=0.2, cv=2, scoring='roc_auc', n_jobs=3)\n",
        "    shap_kwargs={'check_additivity':False}\n",
        "    X_shapRFECV = shapFS_RFECV.fit_compute(FS_X, y, **shap_kwargs)\n",
        "    X_shapRFECV[['num_features', 'features_set', 'val_metric_mean']]\n",
        "    performance_plot = shapFS_RFECV.plot()\n",
        "    print (shapFS_RFECV.get_reduced_features_set(num_features=n))\n",
        "    selected_fs=shapFS_RFECV.get_reduced_features_set(num_features=n)\n",
        "    X_shapRFECV_df=FS_X[selected_fs]\n",
        "    X_shapRFECV_train_df=FS_X_train[selected_fs]\n",
        "    X_shapRFECV_test_df=FS_X_test[selected_fs]\n",
        "    print ('Dataframe with shapRFECV ready' , X_shapRFECV_df.shape)\n",
        "    return X_shapRFECV_df, X_shapRFECV_train_df, X_shapRFECV_test_df\n",
        "\n",
        "def SModelRanTree(FS_X,FS_X_train, FS_X_test,y,n):\n",
        "    FS_SModelRanTree=ExtraTreesClassifier(n_estimators=50)\n",
        "    FS_SModelRanTree.fit(FS_X, y)\n",
        "    model_Tree = SelectFromModel(FS_SModelRanTree, prefit=True)\n",
        "    X_Tree=model_Tree.transform(FS_X)\n",
        "    tree_importance=[]\n",
        "    count = 0\n",
        "    for i in FS_X.columns.values:\n",
        "        tree_importance.insert(count, (i,FS_SModelRanTree.feature_importances_[count]))\n",
        "        count=count+1\n",
        "    tree_importance.sort(key=itemgetter(1), reverse=True)\n",
        "    print ('\\n Selected features for tree classifier')\n",
        "    print ('Dataframe with model selection (random tree):')\n",
        "    ids=[]\n",
        "    values=[]\n",
        "    for i in range( len(tree_importance)):\n",
        "      id, value = tree_importance[i]\n",
        "      ids.append(id)\n",
        "      values.append(value)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.plot(ids, values)\n",
        "    plt.ylabel('Feature importance score')\n",
        "    plt.xlabel('Feature-ID')\n",
        "    plt.show()\n",
        "    res_tree=[]\n",
        "    for i in range(n):\n",
        "        res_tree.append(tree_importance[i][0])\n",
        "    X_SModelRanTree_df=FS_X[res_tree]\n",
        "    X_SModelRanTree_train_df=FS_X_train[res_tree]\n",
        "    X_SModelRanTree_test_df=FS_X_test[res_tree]\n",
        "    return X_SModelRanTree_df, X_SModelRanTree_train_df, X_SModelRanTree_test_df\n",
        "\n",
        "def SModelLassoCV(FS_X,FS_X_train, FS_X_test,y,n):\n",
        "    estimator=LassoCV(cv=5)\n",
        "    iterations=100\n",
        "    v=False\n",
        "    FS_SModelLassoCV=SelectFromModel(estimator, threshold=-1000)\n",
        "    X_LassoCV=FS_SModelLassoCV.fit(FS_X, y)\n",
        "    if verbose:\n",
        "      print('Ranking of features', X_LassoCV.n_features_in_)\n",
        "      print('Rank score of features', X_LassoCV.estimator_.coef_)\n",
        "    high_score=np.max(X_LassoCV.estimator_.coef_)\n",
        "    low_score=np.min(X_LassoCV.estimator_.coef_)\n",
        "    threshold_step=(high_score - low_score)/iterations\n",
        "    start_threshold=high_score+threshold_step\n",
        "    if verbose:\n",
        "      print ('Max/Min/Iterations/step value of coef:',high_score, low_score, iterations,threshold_step)\n",
        "      print ('Starting value of threshold:',start_threshold)\n",
        "    FS_SModelLassoCV.threshold=start_threshold\n",
        "    n_features=FS_SModelLassoCV.transform(FS_X).shape[1]\n",
        "    i=0\n",
        "    for i in range (iterations):\n",
        "      i+=1\n",
        "      FS_SModelLassoCV.threshold-=threshold_step\n",
        "      X_transform = FS_SModelLassoCV.transform(FS_X)\n",
        "      if v:\n",
        "        print ('X_transform',X_transform )\n",
        "      n_features = X_transform.shape[1]\n",
        "      if (n_features>=n):\n",
        "        break\n",
        "    X_LassoCV_mask=FS_SModelLassoCV.get_support()\n",
        "    X_SModelLassoCV_df = FS_X.loc[:, X_LassoCV_mask]\n",
        "    X_SModelLassoCV_train_df = FS_X_train.loc[:, X_LassoCV_mask]\n",
        "    X_SModelLassoCV_test_df = FS_X_test.loc[:, X_LassoCV_mask]\n",
        "    print ('Dataframe with LassoCV (model selection) for n=',n,'best features')\n",
        "    return X_SModelLassoCV_df, X_SModelLassoCV_train_df, X_SModelLassoCV_test_df\n",
        "\n",
        "\n",
        "def GenSelect(FS_X,FS_X_train, FS_X_test,y,n):\n",
        "    FS_XSize=FS_X.shape[1]\n",
        "    if (FS_XSize<n):\n",
        "      n=FS_XSize\n",
        "    estimator = DecisionTreeClassifier()\n",
        "    max_features= n\n",
        "    FS_GenSelect=GeneticSelectionCV(estimator, cv=5, verbose=0, scoring=\"accuracy\", max_features=max_features,\n",
        "    n_population=100, crossover_proba=0.5, mutation_proba=0.2, n_generations=50, crossover_independent_proba=0.5,\n",
        "    mutation_independent_proba=0.04, tournament_size=3, n_gen_no_change=10, caching=True, n_jobs=-1)\n",
        "    X_GenSelect=FS_GenSelect.fit_transform(FS_X, y)\n",
        "    X_GenSelect_mask=FS_GenSelect.get_support()\n",
        "    X_GenSelect_df = FS_X.loc[:, X_GenSelect_mask]\n",
        "    X_GenSelect_train_df = FS_X_train.loc[:, X_GenSelect_mask]\n",
        "    X_GenSelect_test_df = FS_X_test.loc[:, X_GenSelect_mask]\n",
        "    print ('Dataframe with genetic selection for n=',n,'best features')\n",
        "    return X_GenSelect_df, X_GenSelect_train_df, X_GenSelect_test_df\n",
        "\n",
        "\n",
        "def aKDFE_eval_PS (f_selected_in, y ,Normalization, dataset, featuregroup_list, cv_iter=1):\n",
        "    '''\n",
        "    Lightweight script to test many models and find winners\n",
        "    :param X_train: training split\n",
        "    :param y_train: training target vector\n",
        "    :param X_test: test split\n",
        "    :param y_test: test target vector\n",
        "    '''\n",
        "    print_border('#', 50)\n",
        "    print('Number of crossvalidations', cv_iter)\n",
        "    print ('Dataset to be used:', dataset)\n",
        "    print ('Used featuregroup:', featuregroup_list)\n",
        "    print_border('#', 50)\n",
        "    results = []\n",
        "    names = []\n",
        "    Modelrun=0\n",
        "    local_run_report_df= pd.DataFrame(columns=run_report_col)\n",
        "\n",
        "    for fg in featuregroup_list:\n",
        "      for cv in range(1,cv_iter+1):\n",
        "        X_pd, y_pd,X_train, X_test, y_train, y_test=make_csv_df(dataset_import, y, ';', imp_mean, split, ratio, fg, dataset_type)\n",
        "        print ('Normalization', Normalization)\n",
        "        X_train_init=X_train\n",
        "        X_test_init=X_test\n",
        "        y_train_init=y_train\n",
        "        y_test_init= y_test\n",
        "        X_pd_init = X_pd\n",
        "        for k in Normalization:\n",
        "          if (k=='Yes'):\n",
        "            Norm=k\n",
        "            print('Normalization of df performed')\n",
        "            y_train=dfNorm(y_train_init, 'y')\n",
        "            y_test=dfNorm(y_test_init,'y')\n",
        "            X_train_norm=dfNorm(X_train_init, 'X')\n",
        "            X_test_norm=dfNorm(X_test_init, 'X')\n",
        "            X_pd_norm=dfNorm(X_pd_init, 'X')\n",
        "          else:\n",
        "            Norm='No'\n",
        "            print('No normalization of df performed')\n",
        "            y_train=y_train_init\n",
        "            y_test=y_test_init\n",
        "            X_train_norm=X_train_init\n",
        "            X_test_norm=X_test_init\n",
        "            X_pd_norm=X_pd_init\n",
        "          #########################################\n",
        "          #Feature selection\n",
        "          ##########################################\n",
        "          for FS_name, method in selection_methods:\n",
        "              if (Norm == 'No' and fg in ('FE-GEN', 'ORGFE') and FS_name =='KBestN'):\n",
        "                skip_FS_method = True\n",
        "                print ('Skipping the feature selection method:',FS_name, 'for featuregroup', fg, 'and Normalization', Norm )\n",
        "                print_border('-',30)\n",
        "                continue\n",
        "              print ('Selected FS-method:', FS_name)\n",
        "              print ('Size of dataset before FS-method',X_pd_norm.shape, X_train_norm.shape, X_test_norm.shape,y_pd.shape )\n",
        "              if FS_name != 'NO_FS':\n",
        "                X_pd, X_train, X_test=eval(method+'(X_pd_norm, X_train_norm, X_test_norm,y_pd,n)')\n",
        "              else:\n",
        "                 X_pd, X_train, X_test = X_pd_norm, X_train_norm, X_test_norm\n",
        "              this_selected=pd.DataFrame(sorted(X_train), columns=['feature'])\n",
        "              this_selected.insert(0,'Dataset',dataset )\n",
        "              this_selected.insert(1,'Dataset type',dataset_type )\n",
        "              this_selected.insert(2,'FS_method',method )\n",
        "              this_selected.insert(3,'Norm',Norm )\n",
        "              this_selected.insert(4,'Featuregroup',fg )\n",
        "              f_selected_in.append(this_selected)\n",
        "              f_selected_out= pd.concat(f_selected_in, ignore_index=True)\n",
        "              if verbose:\n",
        "                print ('Selected features df:',f_selected_out )\n",
        "              print ('Norm before modeling', Norm)\n",
        "              y_train_arr=np.array(y_train).flatten()\n",
        "              y_test_arr=np.array(y_test).flatten()\n",
        "              for i in (X_train, y_train, X_test, y_test, y_train_arr, y_test_arr):\n",
        "                  print ('Shape of X_train, y_train, X_test, y_test, y_train_arr, y_test_arr:', i.shape)\n",
        "              for name, model in models:\n",
        "                  Modelrun+=1\n",
        "                  print('\\n')\n",
        "                  print_border('#', 50)\n",
        "                  print('Modelrun#',Modelrun,', Starting:',time_diff(starttime, time.time()),', Dataset:', dataset, ', Datseset type:',dataset_type, 'Featuregroup:',fg,', CV#',cv,', Normalization:', Norm,', FS-Method:', FS_name,', Class-Model:', name,'-' ,model)\n",
        "                  clf = model.fit(X_train, y_train_arr)\n",
        "                  y_pred_arr = clf.predict(X_test)\n",
        "                  y_pred = pd.DataFrame(y_pred_arr, columns = ['Y_hat'])\n",
        "                  print('Modelrun#',Modelrun,', Finished:', time_diff(starttime, time.time()),'Featuregroup:',fg,', CV#',cv,', Normalization:', Norm,', FS-Method:', FS_name,', Class-Model:',name  )\n",
        "                  ######################################################################\n",
        "                  #Calculation of ROCAUC for predicted y (y-hat) and accuracy score\n",
        "                  class_rocauc=roc_auc_score(y_test_arr, y_pred_arr)\n",
        "                  test_accur_score=model.score(X_test, y_test_arr)\n",
        "                  if verbose:\n",
        "                    print ('Run#',cv,', Classified ROC_AUC score on (y_test, y_pred):', cv,', for model ',name, ':', class_rocauc,\n",
        "                          ', Norm (Yes/No):', Norm, ', FS-Method: ', FS_name, ', on dataset:', dataset)\n",
        "                  print_border('-', 50)\n",
        "                  ####################################################################\n",
        "                  if dataset_type == 'RAW':\n",
        "                      #Calculation of ROCAUC on majority vote on outcome (Y) group by PID (X)\n",
        "                      #Setting same index on dataframes for join\n",
        "                      X_test_length = len(X_test)\n",
        "                      test_index=pd.Index(range(0, X_test_length, 1))\n",
        "                      X_test=X_test.set_index(test_index)\n",
        "                      y_test=y_test.set_index(test_index)\n",
        "                      result_df=pd.concat([X_test, y_test, y_pred], axis=1, join=\"inner\")\n",
        "                      result_df=add_y_major_vote(result_df, 'PID', 'Y1_hat')\n",
        "                      #Result for unique patients\n",
        "                      result_UnPID_df=make_PID_unique_df(result_df)\n",
        "                      #Arrays for ROC_AUC-value\n",
        "                      y_test_arr_mv=np.array(result_UnPID_df['Y1']).flatten()\n",
        "                      y_pred_arr_mv=np.array(result_UnPID_df['Y_mv']).flatten()\n",
        "                      print('Shape of test/predicted arrays for unique PID (roc_auc_mv) ', y_test_arr_mv.shape, y_pred_arr_mv.shape)\n",
        "                      class_rocauc_mv=roc_auc_score(y_test_arr_mv, y_pred_arr_mv)\n",
        "                  if dataset_type=='TIDY':\n",
        "                      class_rocauc_mv = 0\n",
        "                      X_test_length = len(X_test)\n",
        "                      test_index=pd.Index(range(0, X_test_length, 1))\n",
        "                      X_test=X_test.set_index(test_index)\n",
        "                      y_test=y_test.set_index(test_index)\n",
        "                      result_df=pd.concat([X_test, y_test, y_pred], axis=1, join=\"inner\")\n",
        "\n",
        "                      ####################################################################\n",
        "                  #Adding calculations to result-df\n",
        "                  local_run_report_df.loc[len(local_run_report_df)] = [Modelrun, cv, dataset, dataset_type ,Norm, FS_name, fg, name, test_accur_score, class_rocauc, class_rocauc_mv]\n",
        "                  if reporting:\n",
        "                    print('local run report:')\n",
        "                    display (local_run_report_df)\n",
        "                  if verbose:\n",
        "                    print('Shape and info of result dataframe:', result_df.shape, result_df.columns.values)\n",
        "                    print_border('-', 30)\n",
        "                    for i in (X_test, y_test, y_pred, result_df):\n",
        "                      print ('Shape ... of X_test, y_test, y_pred, result_pd:', i.shape, i.head(1))\n",
        "\n",
        "    return f_selected_out, y_pred, result_df, local_run_report_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOqFyHWPUa0o"
      },
      "outputs": [],
      "source": [
        "\n",
        " #Import files from Drive\n",
        "!cp \"/content/drive/MyDrive/PHD/S14/dataset/S14_TOTAL_DATASET_V2.csv\" \"S14_TOTAL_DATASET_V2.csv\"\n",
        "\n",
        "dataset='S14_TOTAL_DATASET_V2'\n",
        "dataset_import=dataset+'.csv'\n",
        "if dataset=='S14_TOTAL_DATASET_V4':\n",
        "  dataset_type='RAW'\n",
        "elif dataset=='S14_TOTAL_DATASET_V2':\n",
        "    dataset_type='TIDY'\n",
        "print('Dataset type', dataset_type)\n",
        "\n",
        "#Import locally on computer\n",
        "#dataset_import='C:/Users/olbjaa/Documents/PHD/Dataset/'+dataset_import\n",
        "#dataset='C:/Users/olbjaa/Documents/PHD/Dataset/'+dataset_import\n",
        "\n",
        "#dataset_import_test = \"/content/drive/My Drive/PHD/S14/Results/\"\n",
        "print('Dataset_import name:', dataset_import)\n",
        "\n",
        "org_list = [\"10\", \"30\", \"10-E\", \"20-E\", \"30-E\", \"10-M\", \"20-M\", \"30-M\", \"10-S\", \"20-S\", \"30-S\", \"10-W\", \"20-W\", \"30-W\"]\n",
        "idea_list = [\"320\",\"350\",\"400\", \"180-E\", \"180-M\", \"180-S\", \"180-W\", \"215-E\", \"215-M\", \"215-S\", \"215-W\", \"320-E\", \"320-M\", \"320-S\", \"320-W\", \"350-E\", \"350-M\", \"350-S\", \"350-W\", \"400-E\", \"400-M\", \"400-S\", \"400-W\"]\n",
        "y=['Y1']\n",
        "\n",
        "#Init values\n",
        "#featuregroup_list = ['ORG', 'IDEATION', 'FE-GEN', 'ORGIDEA', 'ORGFE']\n",
        "featuregroup_list = ['FE-GEN', 'ORGIDEA', 'ORGFE']\n",
        "\n",
        "result_df = []\n",
        "f_selected = []\n",
        "\n",
        "imp_mean=1\n",
        "split=1\n",
        "ratio=0.3\n",
        "#Numbers of features to select\n",
        "n=5\n",
        "reporting = False\n",
        "verbose=False\n",
        "cv_iter = 2\n",
        "Normalization=['Yes', 'No']\n",
        "#Normalization=['No']\n",
        "print ('Dataset to be used:', dataset)\n",
        "print ('Used featuregroup:', featuregroup_list)\n",
        "print ('Number of cv:', cv_iter)\n",
        "print ('Used normalization:', Normalization)\n",
        "\n",
        "print_border('-')\n",
        "selection_methods = [('NO_FS', 'No Feature selection'),\n",
        "('VT', 'FSVarianceThreshold'),\n",
        "('KBestN','KBestN' ),\n",
        "('KBestMut', 'KBestMutual'),\n",
        "('RFE', 'RFE_FS'),\n",
        "('RFECV', 'RFECV_FS'),\n",
        "('shapRFECV_FS', 'shapRFECV_FS'),\n",
        "('GenSelect', 'GenSelect'),\n",
        "('SModTree','SModelRanTree'),\n",
        "('SModLasso', 'SModelLassoCV')]\n",
        "\n",
        "#selection_methods = [('VT', 'FSVarianceThreshold'), ('KBestN','KBestN')]\n",
        "\n",
        "print('Total feature selection methods to be used:', len(selection_methods))\n",
        "for j in range (len(selection_methods)):\n",
        "  print (selection_methods[j][0])\n",
        "\n",
        "models = [( 'LogReg', LogisticRegression() ),\n",
        "('RF', RandomForestClassifier()),\n",
        "('KNN', KNeighborsClassifier()),\n",
        "#('SVM', SVC()),\n",
        "('GNB', GaussianNB()),\n",
        "('XGB', XGBClassifier()),\n",
        "('ANN-Small',MLPClassifier(hidden_layer_sizes=(15,10,5))),\n",
        "('ANN-Large',MLPClassifier(hidden_layer_sizes=(150,100,50)))\n",
        "]\n",
        "\n",
        "#models = [( 'LogReg', LogisticRegression() )]\n",
        "print_border('-')\n",
        "print('Total classification models to be used:', len(models))\n",
        "for j in range (len(models)):\n",
        "  print (models[j][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmAj512Vhv4Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import normalize\n",
        "\n",
        "########################################################################\n",
        "#Run experiment\n",
        "\n",
        "print ('USED GPU:', tf.test.gpu_device_name())\n",
        "run_report_col=['Model#','CV#','Dataset','Dataset type','Normalizations','FS_method','Featuregroups','Class_modell','Test_accuracy', 'Test_ROC_AUC','Test_ROC_AUC_MV' ]\n",
        "total_run_report_df=pd.DataFrame(columns=run_report_col)\n",
        "\n",
        "\n",
        "starttime=time.time()\n",
        "#starttime=time_diff(0,0)\n",
        "print('Execution started at:',time_diff(0,starttime))\n",
        "\n",
        "print('\\n')\n",
        "print_border('#', 50)\n",
        "exp_selected_f, y_pred, result_df, local_run_report_df =aKDFE_eval_PS(f_selected,y ,Normalization,  dataset,featuregroup_list, cv_iter)\n",
        "#print ('Result after classification for FS-method:',FS, '-- Normalization:',Norm, '-- Dataset:',dataset, end = \"\")\n",
        "#display (exp_result)\n",
        "print ('stop4')\n",
        "total_run_report_df=total_run_report_df.append(local_run_report_df)\n",
        "display (total_run_report_df)\n",
        "display (len(y_pred))\n",
        "print_border('#', 50)\n",
        "#print ('Result after feature selection:',FS, '-- Normalization:',Norm, '-- Dataset:',dataset, end = \"\")\n",
        "display (exp_selected_f)\n",
        "print_border('#', 50)\n",
        "print('Number of selected feature by FS-method')\n",
        "selected_grouped = exp_selected_f.groupby([ 'FS_method', 'Norm']).size()\n",
        "print (selected_grouped)\n",
        "filename_class = dataset+'_'+dataset_type+'_ROCAUC_score_'+actual_time()+'.csv'\n",
        "filename_FS = dataset+'_'+dataset_type+'_selected_features_'+actual_time()+'.csv'\n",
        "local_run_report_df.to_csv(filename_class)\n",
        "exp_selected_f.to_csv(filename_FS)\n",
        "print (filename_class, filename_FS)\n",
        "!cp {filename_class} \"/content/drive/MyDrive/PHD/S14/Results/V5\"\n",
        "!cp {filename_FS} \"/content/drive/MyDrive/PHD/S14/Results/V5\"\n",
        "files.download(filename_class)\n",
        "files.download(filename_FS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VyPVpkiDHGE"
      },
      "outputs": [],
      "source": [
        "#Summary of results and presenting\n",
        "#import pandas as pd\n",
        "pd.set_option('display.max_rows', 100)\n",
        "dataset=\"/content/drive/My Drive/PHD/S10/Results/S1_AED_Fracture_v0_fe_total_selected_features.csv\"\n",
        "S10_total_results = pd.read_csv(dataset,sep=',')\n",
        "dataset=\"/content/drive/My Drive/PHD/S10/Results/S1_AED_Fracture_v0_org_total_selected_features.csv\"\n",
        "S10_total_temp = pd.read_csv(dataset,sep=',')\n",
        "S10_total_results=S10_total_results.append(S10_total_temp)\n",
        "dataset=\"/content/drive/My Drive/PHD/S10/Results/HBA1c_v1_fe_total_selected_features.csv\"\n",
        "S10_total_temp = pd.read_csv(dataset,sep=',')\n",
        "S10_total_results=S10_total_results.append(S10_total_temp)\n",
        "dataset=\"/content/drive/My Drive/PHD/S10/Results/HBA1c_v1_org_total_selected_features.csv\"\n",
        "S10_total_temp = pd.read_csv(dataset,sep=',')\n",
        "S10_total_results=S10_total_results.append(S10_total_temp)\n",
        "print ('Size before feature typing', S10_total_results.shape)\n",
        "var_typing(S10_total_results)\n",
        "\n",
        "print ('Size efter feature typing', S10_total_results.shape)\n",
        "\n",
        "print (S10_total_results.groupby(['dataset', 'FS_method', 'norm']).size())\n",
        "\n",
        "selected_f_summary=S10_total_results[['dataset', 'f_code']].groupby(['dataset',\"f_code\"]).size().reset_index()\n",
        "selected_f_summary.sort_values(by=['dataset', 'f_code'])\n",
        "print (selected_f_summary )\n",
        "display(selected_f_summary.plot(legend=True, figsize=(12,4)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}